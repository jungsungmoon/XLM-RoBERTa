{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f97bfb3",
      "metadata": {
        "id": "3f97bfb3"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import pickle\n",
        "import glob\n",
        "\n",
        "import utils\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "import transformers\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, XLMRobertaConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7af40b24",
      "metadata": {
        "scrolled": true,
        "id": "7af40b24",
        "outputId": "b1b3fa6d-e092-49c2-a105-ec554d0ea1ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "from torch import cuda\n",
        "# GPU 확인\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a01ee2e4",
      "metadata": {
        "scrolled": true,
        "id": "a01ee2e4",
        "outputId": "90c2b343-246f-4582-f5c2-b3bd53054f90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(21974753, 21974751, 4204709)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 현재 특허 데이터가 위치해있는 폴더를 불러온다\n",
        "DATA_DIR = os.path.join('/media', 'eunbinpark', 'eunbin')\n",
        "# 특허 데이터 타입 별로 폴더가 있기에 \n",
        "dir_names = ['ipc_files', 'title_files', 'claim_files']\n",
        "# 이를 데이터 위치와 병합해준다\n",
        "dir_names = [os.path.join(DATA_DIR, dir_name) for dir_name in dir_names] # ex ) '/media/eunbinpark/eunbin/ipc_files'\n",
        "\n",
        "ipc_list = os.listdir(dir_names[0]) # IPC 의 전체 파일 리스트 ex ) ['129749732B1.txt', '2490179012B2.txt']\n",
        "title_list = os.listdir(dir_names[1]) # TITLE 의 전체 파일 리스트 ex ) 상동\n",
        "claim_list = os.listdir(dir_names[2]) # 청구항의 전체 파일 리스트 ex ) 상동\n",
        "len(ipc_list), len(title_list), len(claim_list) # 전체 길이 확인\n",
        "\n",
        "# 길이가 다르기 때문에 Claim 파일 기준으로 진행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52b151d1",
      "metadata": {
        "id": "52b151d1"
      },
      "outputs": [],
      "source": [
        "# 파일 제목을 기준으로 교집합 되는 파일만을 사용\n",
        "intersection_of_files = set(claim_list) & set(title_list) & set(ipc_list)\n",
        "\n",
        "# 기준점의 파일 목록을 전체 파일path를 붙여줌 ex ) \n",
        "claim_list = [os.path.join(DATA_DIR, 'claim_files', filename) for filename in intersection_of_files]\n",
        "# ex ) ['/media/eunbinpark/eunbin/ipc_files/29847179123B2.txt', '/media/eunbinpark/eunbin/ipc_files/29847179123B2.txt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eec8857",
      "metadata": {
        "id": "4eec8857"
      },
      "outputs": [],
      "source": [
        "train_file_list, test_file_list = train_test_split(claim_list, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab986718",
      "metadata": {
        "id": "ab986718",
        "outputId": "30e08863-ff10-40fa-ec83-59623974491e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(340036, 85009)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_file_list), len(test_file_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "359870ac",
      "metadata": {
        "scrolled": true,
        "id": "359870ac",
        "outputId": "430e6c07-e998-498e-8c93-39b140d02859"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "634"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 텍스트 라벨값을 숫자로 바꾸어 미리 저장해둔 dict 파일을 불러옴\n",
        "with open('./dataset/label_encoding_target.pkl', 'rb') as f:\n",
        "    target_dict = pickle.load(f)\n",
        "    \n",
        "    \n",
        "len(target_dict)\n",
        "\n",
        "# ex\n",
        "# { \"G38F\": 234, \"A34Q\" : 394}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d71e67ca",
      "metadata": {
        "id": "d71e67ca"
      },
      "outputs": [],
      "source": [
        "class DataTransform(Dataset):\n",
        "\n",
        "    def __init__(self, file_list, target_dict, tokenizer, max_len):\n",
        "        self.file_list = file_list\n",
        "        self.target_dict = target_dict\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        # 파일을 열기 위한 함수\n",
        "        def _file_open(filepath, data_type='list'): \n",
        "            if data_type == \"str\": \n",
        "                with open(filepath, 'r') as f:\n",
        "                    data = f.read()\n",
        "                    \n",
        "            elif data_type == \"ipc\":\n",
        "                data = list()\n",
        "                with open(filepath, 'r') as f:\n",
        "                    for line in f:\n",
        "                        # IPC 파일 형태 -> [\"A39V 01/03\", \"A39V 01/03\", \"A39V 01/03\"]\n",
        "                        string_value = line.strip().replace(\" \", \"\")[:4] # 해당 형태에서 앞 4글자만 추출\n",
        "                        data.append(self.target_dict[string_value]) # 출력을 위한 리스트에 삽입\n",
        "                data = data[:5]\n",
        "\n",
        "\n",
        "            else:\n",
        "                data = list()\n",
        "                with open(filepath, 'r') as f:\n",
        "                    for line in f:        \n",
        "                        data.append(line.strip())\n",
        "\n",
        "            return data\n",
        "        \n",
        "        # idx 번호를 기준으로 클레임 파일 불러오기\n",
        "        y_data = self.file_list[idx]\n",
        "        # 청구항 파일 path 기준으로 파일명만 추출\n",
        "        filename = self.file_list[idx].split('/')[-1]\n",
        "\n",
        "        # 패스 삽입\n",
        "        title_filepath = os.path.join('/media', 'eunbinpark', 'eunbin', 'title_files', filename)\n",
        "        ipc_filepath = os.path.join('/media', 'eunbinpark', 'eunbin', 'ipc_files', filename)\n",
        "        \n",
        "        # 미리 만들어놓은 함수로 파일 열기\n",
        "        title_data = _file_open(title_filepath, 'str')\n",
        "        claim_data = _file_open(y_data)\n",
        "        targets = _file_open(ipc_filepath, 'ipc')\n",
        "\n",
        "        # 전체 IPC 코드 개수를 기준으로 0으로 채워진 array 생성\n",
        "        one_hot = np.zeros(len(target_dict))        \n",
        "        for i in targets: # idx 를 기준으로 해당 array의 idx에 1로 표기 \n",
        "            one_hot[i] = 1\n",
        "        # ex ) [0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,1]\n",
        "        \n",
        "        \n",
        "        # 제목과 청구항을 하나로 이어 붙임\n",
        "        all_text = title_data + ' ' + ' '.join(claim_data)\n",
        "            \n",
        "        # 이어 붙인 텍스트를 모델에 맞는 토크나이저를 사용해 \n",
        "        # special token 삽입, MAX_LEN보다 긴 문장 절삭 등 진행\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            all_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask = True, \n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'one_hot': torch.tensor(one_hot, dtype=torch.long),\n",
        "\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d4d7ea6",
      "metadata": {
        "id": "9d4d7ea6"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 256\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "VALID_BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 3e-05\n",
        "MODEL_NAME = 'xlm-roberta-base'\n",
        "\n",
        "# 사용할 모델 xlm-roberta의 전용 토크나이저 사용\n",
        "xlmroberta_tokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ff76564",
      "metadata": {
        "id": "2ff76564"
      },
      "outputs": [],
      "source": [
        "# 트레인, 테스트 나눈 파일을 데이터 로더에 태움\n",
        "training = DataTransform(train_file_list, target_dict, xlmroberta_tokenizer, MAX_LEN)\n",
        "loaded_train_data = DataLoader(training, **train_params)\n",
        "\n",
        "\n",
        "testing = DataTransform(test_file_list, target_dict, xlmroberta_tokenizer, MAX_LEN)\n",
        "loaded_test_data = DataLoader(testing, **test_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07dc90ba",
      "metadata": {
        "id": "07dc90ba",
        "outputId": "26c3a769-fb6c-40e3-cc7a-f60542d27c9c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "XLMRobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=634, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 모델 호출\n",
        "# num_labels -> y 개수\n",
        "model = XLMRobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(target_dict))\n",
        "# model.load_state_dict(torch.load('./model/20211215-xlm-roberta-base.pt.pt'))\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6948c731",
      "metadata": {
        "id": "6948c731"
      },
      "outputs": [],
      "source": [
        "# 모델을 돌리는 시각 호출 \n",
        "import datetime\n",
        "import pytz\n",
        "timezone = pytz.timezone('Asia/Seoul')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d069c66f",
      "metadata": {
        "id": "d069c66f"
      },
      "outputs": [],
      "source": [
        "# BCEWithLogitsLoss 사용 \n",
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.functional.binary_cross_entropy_with_logits(outputs, targets)\n",
        "\n",
        "\n",
        "# 옵티마이저 아담, 러닝레이트 3e-05 사용\n",
        "optimizer = torch.optim.Adam(params =model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b5a3dbb",
      "metadata": {
        "scrolled": true,
        "id": "2b5a3dbb",
        "outputId": "fab50235-8ca8-4e2d-d6ac-be3f51c7406a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2021-12-18 21:54:00.194246+09:00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/eunbinpark/miniconda3/envs/pytorch-gpu/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2149: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 1000 steps: 0.6958234906196594\n",
            "Training Accuracy per 1000 steps: 2.127659574468085\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.06134167618093106\n",
            "Training Accuracy per 1000 steps: 3.825022019041228\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.03827388159584427\n",
            "Training Accuracy per 1000 steps: 3.8107395471477004\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.030441774089498864\n",
            "Training Accuracy per 1000 steps: 3.7592780849197176\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.026457073519151157\n",
            "Training Accuracy per 1000 steps: 3.6173440033474553\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.024077481990413794\n",
            "Training Accuracy per 1000 steps: 3.5273744839442345\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.02249498309052635\n",
            "Training Accuracy per 1000 steps: 3.5655063820299873\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.021318708734559964\n",
            "Training Accuracy per 1000 steps: 3.69120999620541\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.020350002783803795\n",
            "Training Accuracy per 1000 steps: 4.304738794945076\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.01943727919297766\n",
            "Training Accuracy per 1000 steps: 5.557802259099149\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.018577374310253466\n",
            "Training Accuracy per 1000 steps: 7.134951933638085\n",
            "--------------------------------------------------\n",
            "==================================================\n",
            "The Total Accuracy for Epoch 0: 8.251749353531892\n",
            "Training Loss Epoch: 0.018068093908997135\n",
            "Training Precision Top 5 Epoch: 8.251749353531892\n",
            "==================================================\n",
            "Training Loss per 1000 steps: 0.008848506025969982\n",
            "Training Accuracy per 1000 steps: 31.11111111111111\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.009320466055792499\n",
            "Training Accuracy per 1000 steps: 28.974128750704725\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.00899465212403894\n",
            "Training Accuracy per 1000 steps: 30.387931034482758\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.008756110629160548\n",
            "Training Accuracy per 1000 steps: 31.336395892731417\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.008526999259894117\n",
            "Training Accuracy per 1000 steps: 32.25430261320671\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.008329189627103014\n",
            "Training Accuracy per 1000 steps: 33.025834887552584\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.008165257784415833\n",
            "Training Accuracy per 1000 steps: 33.69098712446352\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0080165468334461\n",
            "Training Accuracy per 1000 steps: 34.2870115361408\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.007872752266659087\n",
            "Training Accuracy per 1000 steps: 34.82959138029805\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.007746726408792845\n",
            "Training Accuracy per 1000 steps: 35.33684249683803\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.007629063706399917\n",
            "Training Accuracy per 1000 steps: 35.789348106816135\n",
            "--------------------------------------------------\n",
            "==================================================\n",
            "The Total Accuracy for Epoch 1: 36.06494419596699\n",
            "Training Loss Epoch: 0.007561523408629088\n",
            "Training Precision Top 5 Epoch: 36.06494419596699\n",
            "==================================================\n",
            "Training Loss per 1000 steps: 0.00866707507520914\n",
            "Training Accuracy per 1000 steps: 37.254901960784316\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0061881982320854505\n",
            "Training Accuracy per 1000 steps: 41.54089227059166\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0061491303617597325\n",
            "Training Accuracy per 1000 steps: 41.70768332360057\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.006104102306628413\n",
            "Training Accuracy per 1000 steps: 41.97068585613477\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.006066187292219169\n",
            "Training Accuracy per 1000 steps: 42.1309821834545\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0060186224454282745\n",
            "Training Accuracy per 1000 steps: 42.33610554204563\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0059832745235800676\n",
            "Training Accuracy per 1000 steps: 42.46382727808772\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.005951782444027984\n",
            "Training Accuracy per 1000 steps: 42.595708688128205\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.005920924313241632\n",
            "Training Accuracy per 1000 steps: 42.70026532132635\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.005888905454909789\n",
            "Training Accuracy per 1000 steps: 42.81484580166864\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.005862438018606865\n",
            "Training Accuracy per 1000 steps: 42.93225154506312\n",
            "--------------------------------------------------\n",
            "==================================================\n",
            "The Total Accuracy for Epoch 2: 43.00997126589583\n",
            "Training Loss Epoch: 0.005844549582534116\n",
            "Training Precision Top 5 Epoch: 43.00997126589583\n",
            "==================================================\n",
            "Training Loss per 1000 steps: 0.003732427954673767\n",
            "Training Accuracy per 1000 steps: 42.857142857142854\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.005180942442418857\n",
            "Training Accuracy per 1000 steps: 45.617251797062195\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.005161707011306996\n",
            "Training Accuracy per 1000 steps: 45.73884704972999\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.005171277979792272\n",
            "Training Accuracy per 1000 steps: 45.828836419086876\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.005173158964516501\n",
            "Training Accuracy per 1000 steps: 45.919581624025305\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0051526310983553624\n",
            "Training Accuracy per 1000 steps: 45.947696035426326\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.005139523139813196\n",
            "Training Accuracy per 1000 steps: 46.008676411659145\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0051351706439836985\n",
            "Training Accuracy per 1000 steps: 46.00477389711923\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.005126431714377877\n",
            "Training Accuracy per 1000 steps: 46.08382062198604\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.005112321454476627\n",
            "Training Accuracy per 1000 steps: 46.12789681211174\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.005101246122497484\n",
            "Training Accuracy per 1000 steps: 46.15854372668105\n",
            "--------------------------------------------------\n",
            "==================================================\n",
            "The Total Accuracy for Epoch 3: 46.192386545585606\n",
            "Training Loss Epoch: 0.005090665859186471\n",
            "Training Precision Top 5 Epoch: 46.192386545585606\n",
            "==================================================\n",
            "Training Loss per 1000 steps: 0.0043610162101686\n",
            "Training Accuracy per 1000 steps: 54.166666666666664\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0045555837377515085\n",
            "Training Accuracy per 1000 steps: 48.55388727629628\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.004556851512610518\n",
            "Training Accuracy per 1000 steps: 48.58555802355308\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 1000 steps: 0.0045631206550960325\n",
            "Training Accuracy per 1000 steps: 48.490662127374584\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.004579798658526233\n",
            "Training Accuracy per 1000 steps: 48.34918126298492\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0045737958391873215\n",
            "Training Accuracy per 1000 steps: 48.357202680067005\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.004581782282482963\n",
            "Training Accuracy per 1000 steps: 48.335077438258686\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.004584135009376807\n",
            "Training Accuracy per 1000 steps: 48.319528750411145\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.004586153887038839\n",
            "Training Accuracy per 1000 steps: 48.304049831202555\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.00458756270870664\n",
            "Training Accuracy per 1000 steps: 48.284864730402354\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.004584623242311873\n",
            "Training Accuracy per 1000 steps: 48.30039327147871\n",
            "--------------------------------------------------\n",
            "==================================================\n",
            "The Total Accuracy for Epoch 4: 48.289050514121854\n",
            "Training Loss Epoch: 0.0045835972512560655\n",
            "Training Precision Top 5 Epoch: 48.289050514121854\n",
            "==================================================\n",
            "Training Loss per 1000 steps: 0.004887198098003864\n",
            "Training Accuracy per 1000 steps: 46.2962962962963\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.004051957708121939\n",
            "Training Accuracy per 1000 steps: 50.74614335187323\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.004109943144443235\n",
            "Training Accuracy per 1000 steps: 50.400996733394756\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.004130023032282838\n",
            "Training Accuracy per 1000 steps: 50.27541102480791\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.004138721884685687\n",
            "Training Accuracy per 1000 steps: 50.22779460302017\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.004144850054270126\n",
            "Training Accuracy per 1000 steps: 50.17159981249581\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.004152263778314917\n",
            "Training Accuracy per 1000 steps: 50.15346731494981\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.004157638065649311\n",
            "Training Accuracy per 1000 steps: 50.106704127686285\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.00416012968481101\n",
            "Training Accuracy per 1000 steps: 50.08188187035981\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.00416577302955752\n",
            "Training Accuracy per 1000 steps: 50.049783418864656\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.004168393798159902\n",
            "Training Accuracy per 1000 steps: 50.06040404759462\n",
            "--------------------------------------------------\n",
            "==================================================\n",
            "The Total Accuracy for Epoch 5: 50.028064495164045\n",
            "Training Loss Epoch: 0.004174671818790048\n",
            "Training Precision Top 5 Epoch: 50.028064495164045\n",
            "==================================================\n",
            "Training Loss per 1000 steps: 0.003444541245698929\n",
            "Training Accuracy per 1000 steps: 50.0\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.003686513527869605\n",
            "Training Accuracy per 1000 steps: 52.11267605633803\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.003712420643743375\n",
            "Training Accuracy per 1000 steps: 52.029895491567\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0037378690056397304\n",
            "Training Accuracy per 1000 steps: 51.85883733601576\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0037582385181434224\n",
            "Training Accuracy per 1000 steps: 51.73302058681857\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.003769747215555748\n",
            "Training Accuracy per 1000 steps: 51.67015487651737\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0037862431972284956\n",
            "Training Accuracy per 1000 steps: 51.54993899250479\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.003791611595268866\n",
            "Training Accuracy per 1000 steps: 51.55271890227497\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.003795302806249492\n",
            "Training Accuracy per 1000 steps: 51.511776562953436\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.00380563065621254\n",
            "Training Accuracy per 1000 steps: 51.47057456735506\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0038099065760233246\n",
            "Training Accuracy per 1000 steps: 51.457322175732216\n",
            "--------------------------------------------------\n",
            "==================================================\n",
            "The Total Accuracy for Epoch 6: 51.458073613663174\n",
            "Training Loss Epoch: 0.003813372203394286\n",
            "Training Precision Top 5 Epoch: 51.458073613663174\n",
            "==================================================\n",
            "Training Loss per 1000 steps: 0.004396145232021809\n",
            "Training Accuracy per 1000 steps: 45.833333333333336\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.003390144425226593\n",
            "Training Accuracy per 1000 steps: 52.898444838743345\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0034073636206662814\n",
            "Training Accuracy per 1000 steps: 52.97335100886212\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.003413166417225125\n",
            "Training Accuracy per 1000 steps: 53.030060036398375\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0034273610129625155\n",
            "Training Accuracy per 1000 steps: 52.86945433070455\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.003433354884411948\n",
            "Training Accuracy per 1000 steps: 52.88993762002184\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.003452798711619259\n",
            "Training Accuracy per 1000 steps: 52.792063890282655\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0034627457618546563\n",
            "Training Accuracy per 1000 steps: 52.777678144428506\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0034721258900638954\n",
            "Training Accuracy per 1000 steps: 52.74174728833868\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0034797200366089827\n",
            "Training Accuracy per 1000 steps: 52.720850357299824\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.003492568684991424\n",
            "Training Accuracy per 1000 steps: 52.67837339036036\n",
            "--------------------------------------------------\n",
            "==================================================\n",
            "The Total Accuracy for Epoch 7: 52.65549207399573\n",
            "Training Loss Epoch: 0.003498083757561472\n",
            "Training Precision Top 5 Epoch: 52.65549207399573\n",
            "==================================================\n",
            "Training Loss per 1000 steps: 0.0035660730209201574\n",
            "Training Accuracy per 1000 steps: 50.943396226415096\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0030968085380985617\n",
            "Training Accuracy per 1000 steps: 54.153678793330684\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.003102293138325713\n",
            "Training Accuracy per 1000 steps: 54.139315131496424\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.003110402858889327\n",
            "Training Accuracy per 1000 steps: 54.18198305025634\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.003132810465584378\n",
            "Training Accuracy per 1000 steps: 54.155066669457646\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0031398574201886627\n",
            "Training Accuracy per 1000 steps: 54.06987945751694\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss per 1000 steps: 0.0031529968568013533\n",
            "Training Accuracy per 1000 steps: 54.02933406848803\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0031700769074745672\n",
            "Training Accuracy per 1000 steps: 53.91292132312723\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0031801950942484925\n",
            "Training Accuracy per 1000 steps: 53.88137770788469\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0031899827637335842\n",
            "Training Accuracy per 1000 steps: 53.83738805050315\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0031973205939427526\n",
            "Training Accuracy per 1000 steps: 53.780250266229544\n",
            "--------------------------------------------------\n",
            "==================================================\n",
            "The Total Accuracy for Epoch 8: 53.7623163745005\n",
            "Training Loss Epoch: 0.0032035259744791302\n",
            "Training Precision Top 5 Epoch: 53.7623163745005\n",
            "==================================================\n",
            "Training Loss per 1000 steps: 0.0019616656936705112\n",
            "Training Accuracy per 1000 steps: 61.36363636363637\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.002801798154496691\n",
            "Training Accuracy per 1000 steps: 55.26304801670146\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.002821427699039294\n",
            "Training Accuracy per 1000 steps: 55.234139604954805\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0028389733416915577\n",
            "Training Accuracy per 1000 steps: 55.1857639131102\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.002849025187082356\n",
            "Training Accuracy per 1000 steps: 55.125113132824474\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0028621085884149725\n",
            "Training Accuracy per 1000 steps: 55.043716396160995\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.002873835184761453\n",
            "Training Accuracy per 1000 steps: 54.987214060695706\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.002884426477003096\n",
            "Training Accuracy per 1000 steps: 54.9354079364937\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0028963668818022545\n",
            "Training Accuracy per 1000 steps: 54.8957291628489\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0029081479663751733\n",
            "Training Accuracy per 1000 steps: 54.865513046269655\n",
            "--------------------------------------------------\n",
            "Training Loss per 1000 steps: 0.0029196856952138353\n",
            "Training Accuracy per 1000 steps: 54.80446331241723\n",
            "--------------------------------------------------\n",
            "==================================================\n",
            "The Total Accuracy for Epoch 9: 54.76436655972617\n",
            "Training Loss Epoch: 0.0029274929674099346\n",
            "Training Precision Top 5 Epoch: 54.76436655972617\n",
            "==================================================\n",
            "2021-12-19 19:14:10.203568+09:00\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def train(epoch, num=1000):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    model.train()\n",
        "    for _, data in enumerate(loaded_train_data, 0):\n",
        "        # 데이터로더로 만들어진 데이터를 각각 불러와 gpu위에 올리기\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        one_hot = data['one_hot'].to(device, dtype = torch.float32)\n",
        "\n",
        "        # 원핫인코딩에서 원 인덱스를 호출 \n",
        "        targets = list()\n",
        "        for target in one_hot.cpu():\n",
        "            where_ones = np.argwhere(target == 1)[0].tolist()\n",
        "            targets.append(where_ones)\n",
        "\n",
        "        # 일열로 리스트 데이터를 펼침\n",
        "        targets = torch.FloatTensor(list(itertools.chain(*targets)))\n",
        "\n",
        "        # 원핫인코딩을 기준으로 1의 갯수가 몇 개인지 카운트\n",
        "        num_target = torch.count_nonzero(one_hot, dim=1) # num_target : targets에서 \n",
        "\n",
        "        # 모델에 태워 prediction 진행\n",
        "        outputs = model(ids, mask)\n",
        "        # 로스 계산\n",
        "        loss = loss_fn(outputs.logits, one_hot)\n",
        "\n",
        "        tr_loss += loss.item()\n",
        "        \n",
        "        big_val, big_idx = outputs.logits.topk(1, 1, True, True)\n",
        "\n",
        "        big_idx = torch.FloatTensor(list(itertools.chain(*big_idx)))\n",
        "        big_idx = big_idx.numpy()\n",
        "        targets = targets.numpy()\n",
        "        correct = len(set(targets) & set(big_idx))\n",
        "        n_correct += correct\n",
        "\n",
        "        logits=outputs.logits\n",
        "        \n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size\n",
        "        \n",
        "        if _%num==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            accu_step = (n_correct*100)/nb_tr_examples \n",
        "            print(f\"Training Loss per {num} steps: {loss_step}\")\n",
        "            print(f\"Training Accuracy per {num} steps: {accu_step}\")\n",
        "            print('-'*50)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # # When using GPU\n",
        "        optimizer.step()\n",
        "    print('='*50)\n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Training Precision Top 5 Epoch: {epoch_accu}\")\n",
        "    print('='*50)\n",
        "\n",
        "    return \n",
        "\n",
        "\n",
        "start_time = datetime.datetime.now(timezone)# .strftime('%Y%m%d')\n",
        "print(start_time)\n",
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)\n",
        "    \n",
        "end_time = datetime.datetime.now(timezone) # .strftime('%Y%m%d')   \n",
        "print(end_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48685e8a",
      "metadata": {
        "id": "48685e8a"
      },
      "outputs": [],
      "source": [
        "# 최종 결과값\n",
        "\n",
        "# ==================================================\n",
        "# The Total Accuracy for Epoch 9: 83.00673028074998\n",
        "# Training Loss Epoch: 0.0029771517266332613\n",
        "# Training Precision Top 5 Epoch: 83.00673028074998\n",
        "# =================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e777d7a",
      "metadata": {
        "id": "0e777d7a"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "055860a3",
      "metadata": {
        "id": "055860a3",
        "outputId": "6d4ecf12-8cab-49f5-dccb-3840034d0567"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'./model/20211219-xlm-roberta-base'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 모델 저장용 파일 이름 생성 \n",
        "FILENAME = f\"./model/{datetime.datetime.now(timezone).strftime('%Y%m%d')}-{MODEL_NAME}\"\n",
        "FILENAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f4ff7f6",
      "metadata": {
        "id": "8f4ff7f6"
      },
      "outputs": [],
      "source": [
        "# 모델 저장\n",
        "torch.save(model.state_dict(), f\"{FILENAME}.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a6f4f10",
      "metadata": {
        "id": "4a6f4f10"
      },
      "outputs": [],
      "source": [
        "# 학습된 모델 성능 측정\n",
        "\n",
        "def validation(epoch):\n",
        "    model.eval()\n",
        "    t_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_t_steps = 0\n",
        "    nb_t_recall = 0\n",
        "    nb_t_precision = 0\n",
        "    \n",
        "    # 학습된 파라미터 값을 평가하는 단계에서는 gradient를 계산할 필요가 없기 때문에 \n",
        "    # 메모리 사용량을 줄이기 위해 코드 블럭을 with torch.no_grad():로 감싼다\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loaded_test_data, 0):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            one_hot = data['one_hot'].to(device, dtype = torch.float) # 정답\n",
        "            \n",
        "            targets = list()\n",
        "            for target in one_hot.cpu():\n",
        "                where_ones = np.argwhere(target == 1)[0].tolist()\n",
        "                targets.append(where_ones)\n",
        "\n",
        "            targets = torch.FloatTensor(list(itertools.chain(*targets)))\n",
        "#             model.eval()\n",
        "            outputs = model(ids, mask) # 예측값\n",
        "#             model.train()\n",
        "            big_val, big_idx = outputs.logits.topk(1, 1, True, True)\n",
        "\n",
        "            big_idx = torch.FloatTensor(list(itertools.chain(*big_idx)))\n",
        "            big_idx = big_idx.numpy()\n",
        "            targets = targets.numpy()\n",
        "            \n",
        "            correct = len(set(targets) & set(big_idx))\n",
        "            n_correct += correct\n",
        "\n",
        "            nb_t_steps += 1\n",
        "            nb_t_recall+=targets.size\n",
        "            nb_t_precision+=big_idx.size\n",
        "            \n",
        "            if _%50==0:\n",
        "                recall = (n_correct*100)/nb_t_recall \n",
        "                precision = (n_correct*100)/nb_t_precision \n",
        "                f1 = (2*recall*precision)/(recall+precision)\n",
        "                \n",
        "                print(f\"Validation recall per 500 steps: {recall}\")\n",
        "                print(f\"Validation precision per 500 steps: {precision}\")\n",
        "                print(f\"Validation f1 per 500 steps: {f1}\")\n",
        "                print(\"-\" * 30)\n",
        "            \n",
        "\n",
        "        epoch_recall = (n_correct*100)/nb_t_recall\n",
        "        epoch_precision = (n_correct*100)/nb_t_precision\n",
        "        epoch_f1 = (2*epoch_recall*epoch_precision) / (epoch_recall + epoch_precision)\n",
        "            \n",
        "    return epoch_recall, epoch_precision, epoch_f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5c6c5ac",
      "metadata": {
        "scrolled": true,
        "id": "c5c6c5ac",
        "outputId": "148589bd-4158-4e49-b5a3-fdce89c5b855"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation recall per 500 steps: 38.297872340425535\n",
            "Validation precision per 500 steps: 56.25\n",
            "Validation f1 per 500 steps: 45.569620253164565\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 47.39005343197698\n",
            "Validation precision per 500 steps: 70.64950980392157\n",
            "Validation f1 per 500 steps: 56.72816728167281\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 47.573620904189134\n",
            "Validation precision per 500 steps: 70.97772277227723\n",
            "Validation f1 per 500 steps: 56.96548298981873\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 47.77361631294215\n",
            "Validation precision per 500 steps: 71.2748344370861\n",
            "Validation f1 per 500 steps: 57.204551117016855\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.18848167539267\n",
            "Validation precision per 500 steps: 71.54850746268657\n",
            "Validation f1 per 500 steps: 57.58978851207609\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.18731117824773\n",
            "Validation precision per 500 steps: 71.4890438247012\n",
            "Validation f1 per 500 steps: 57.56968117104472\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.16340782122905\n",
            "Validation precision per 500 steps: 71.60506644518273\n",
            "Validation f1 per 500 steps: 57.59018036072145\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.174090038314176\n",
            "Validation precision per 500 steps: 71.64351851851852\n",
            "Validation f1 per 500 steps: 57.6102520045819\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.22401508801341\n",
            "Validation precision per 500 steps: 71.73472568578553\n",
            "Validation f1 per 500 steps: 57.675438596491226\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.2829413960433\n",
            "Validation precision per 500 steps: 71.70177383592018\n",
            "Validation f1 per 500 steps: 57.70689270577738\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.336907615323156\n",
            "Validation precision per 500 steps: 71.70034930139721\n",
            "Validation f1 per 500 steps: 57.74495767714063\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.368463333460845\n",
            "Validation precision per 500 steps: 71.71052631578948\n",
            "Validation f1 per 500 steps: 57.770771937038816\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.458736163654194\n",
            "Validation precision per 500 steps: 71.93219633943427\n",
            "Validation f1 per 500 steps: 57.90707408957722\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.421290760430466\n",
            "Validation precision per 500 steps: 71.92300307219662\n",
            "Validation f1 per 500 steps: 57.87735393529696\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.41555635577962\n",
            "Validation precision per 500 steps: 71.92403708987162\n",
            "Validation f1 per 500 steps: 57.87359207977617\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.37897563568212\n",
            "Validation precision per 500 steps: 71.96654460719041\n",
            "Validation f1 per 500 steps: 57.861193355748476\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.40266484813513\n",
            "Validation precision per 500 steps: 71.99594257178526\n",
            "Validation f1 per 500 steps: 57.88763762978763\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.41171820966308\n",
            "Validation precision per 500 steps: 71.9704759106933\n",
            "Validation f1 per 500 steps: 57.88587630692894\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.32813300423788\n",
            "Validation precision per 500 steps: 71.98598779134295\n",
            "Validation f1 per 500 steps: 57.83109030622195\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.301511974395765\n",
            "Validation precision per 500 steps: 71.90786014721346\n",
            "Validation f1 per 500 steps: 57.78681489892654\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.230338844533854\n",
            "Validation precision per 500 steps: 71.85314685314685\n",
            "Validation f1 per 500 steps: 57.71820495780723\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.193925840138974\n",
            "Validation precision per 500 steps: 71.76498572787821\n",
            "Validation f1 per 500 steps: 57.66368425453633\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.23208737586489\n",
            "Validation precision per 500 steps: 71.82107175295187\n",
            "Validation f1 per 500 steps: 57.70910542220195\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.21454625036475\n",
            "Validation precision per 500 steps: 71.77725890529975\n",
            "Validation f1 per 500 steps: 57.682405306336186\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.205146121194126\n",
            "Validation precision per 500 steps: 71.80474604496253\n",
            "Validation f1 per 500 steps: 57.68454937865153\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.17701785265275\n",
            "Validation precision per 500 steps: 71.79256594724221\n",
            "Validation f1 per 500 steps: 57.66047729393001\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.19832897835414\n",
            "Validation precision per 500 steps: 71.77651806302843\n",
            "Validation f1 per 500 steps: 57.6705587185178\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.20160651305874\n",
            "Validation precision per 500 steps: 71.76165803108809\n",
            "Validation f1 per 500 steps: 57.668107254054554\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.211797491871806\n",
            "Validation precision per 500 steps: 71.77462526766595\n",
            "Validation f1 per 500 steps: 57.67958772126372\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.15741517456828\n",
            "Validation precision per 500 steps: 71.71131977946244\n",
            "Validation f1 per 500 steps: 57.62022600238809\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.11656999091481\n",
            "Validation precision per 500 steps: 71.67096935376416\n",
            "Validation f1 per 500 steps: 57.57796231716802\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.10696749584061\n",
            "Validation precision per 500 steps: 71.65739845261122\n",
            "Validation f1 per 500 steps: 57.56670794182631\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.0693438780721\n",
            "Validation precision per 500 steps: 71.65833853841349\n",
            "Validation f1 per 500 steps: 57.54006504447319\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.0516512398583\n",
            "Validation precision per 500 steps: 71.63272259236827\n",
            "Validation f1 per 500 steps: 57.519131247577725\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.040218342225565\n",
            "Validation precision per 500 steps: 71.62698412698413\n",
            "Validation f1 per 500 steps: 57.509089970425336\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 47.99593082400814\n",
            "Validation precision per 500 steps: 71.57160194174757\n",
            "Validation f1 per 500 steps: 57.45950554134698\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 47.985522763243644\n",
            "Validation precision per 500 steps: 71.54532204330927\n",
            "Validation f1 per 500 steps: 57.44357759821677\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 47.98224054546896\n",
            "Validation precision per 500 steps: 71.52215018908699\n",
            "Validation f1 per 500 steps: 57.43375609227036\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.01293084425615\n",
            "Validation precision per 500 steps: 71.53636244082062\n",
            "Validation f1 per 500 steps: 57.46032165209813\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.00666594989786\n",
            "Validation precision per 500 steps: 71.51941312147616\n",
            "Validation f1 per 500 steps: 57.450367339586464\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.00360598341667\n",
            "Validation precision per 500 steps: 71.51736631684157\n",
            "Validation f1 per 500 steps: 57.447515822090075\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 48.010224948875255\n",
            "Validation precision per 500 steps: 71.54132130667966\n",
            "Validation f1 per 500 steps: 57.45998335699251\n",
            "------------------------------\n",
            "Validation recall per 500 steps: 47.997323773479394\n",
            "Validation precision per 500 steps: 71.49125416468348\n",
            "Validation f1 per 500 steps: 57.43459219826376\n",
            "------------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_3701/2577321033.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total Recall Score = {recall}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total Precision = {precision}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_3701/149102762.py\u001b[0m in \u001b[0;36mvalidation\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# 메모리 사용량을 줄이기 위해 코드 블럭을 with torch.no_grad():로 감싼다\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_test_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_3701/3033042613.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# 미리 만들어놓은 함수로 파일 열기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mtitle_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_file_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'str'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mclaim_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_file_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_file_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipc_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ipc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_3701/3033042613.py\u001b[0m in \u001b[0;36m_file_open\u001b[0;34m(filepath, data_type)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_file_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'list'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"str\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.8/_bootlocale.py\u001b[0m in \u001b[0;36mgetpreferredencoding\u001b[0;34m(do_setlocale)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_setlocale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_setlocale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdo_setlocale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutf8_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    recall, precision, f1 = validation(epoch)\n",
        "    print(\"=\" * 30)\n",
        "    print(f\"Total Recall Score = {recall}\")\n",
        "    print(f\"Total Precision = {precision}\")\n",
        "    print(f\"Total F1 Score = {f1}\")\n",
        "    print(\"=\" * 30)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "model-desc-2.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}